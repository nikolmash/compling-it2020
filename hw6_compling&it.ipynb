{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hw6_compling&it.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMxmrPcTr6f9GOmKLWJlJQc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikolmash/compling-it2020/blob/master/hw6_compling%26it.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vSNKJf3nTwE",
        "outputId": "ad5207b1-6c58-4317-8c38-db806696a96b"
      },
      "source": [
        "!pip install gensim --upgrade -q\r\n",
        "!pip install pymorphy2 -q\r\n",
        "\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import pymorphy2\r\n",
        "import re\r\n",
        "from string import punctuation\r\n",
        "from gensim.models import FastText\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "%tensorflow_version 1.x\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Bidirectional, TimeDistributed, Conv1D, Flatten, concatenate\r\n",
        "from keras.models import Model\r\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35Z3Gud0umUK",
        "outputId": "f87920b6-85d7-48e1-cf6f-00b8e97a6150"
      },
      "source": [
        "!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\r\n",
        "!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-21 14:46:08--  https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.18, 2620:100:601c:18::a27d:612\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/r6u59ljhhjdg6j0/negative.csv [following]\n",
            "--2021-02-21 14:46:09--  https://www.dropbox.com/s/raw/r6u59ljhhjdg6j0/negative.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uce5dd90142fbff04ab3f13be51c.dl.dropboxusercontent.com/cd/0/inline/BJXsnGYELOWO62vL0JYaZ8OyXfSnnBqn19nSDfNAZbblC4E5oOcfZyiXhW-qSTTdFJNwX0iE9HXr3EwwWIxM6dFmW2_HdntX2Rr-H5BT7nJJXg/file# [following]\n",
            "--2021-02-21 14:46:09--  https://uce5dd90142fbff04ab3f13be51c.dl.dropboxusercontent.com/cd/0/inline/BJXsnGYELOWO62vL0JYaZ8OyXfSnnBqn19nSDfNAZbblC4E5oOcfZyiXhW-qSTTdFJNwX0iE9HXr3EwwWIxM6dFmW2_HdntX2Rr-H5BT7nJJXg/file\n",
            "Resolving uce5dd90142fbff04ab3f13be51c.dl.dropboxusercontent.com (uce5dd90142fbff04ab3f13be51c.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:6019:15::a27d:40f\n",
            "Connecting to uce5dd90142fbff04ab3f13be51c.dl.dropboxusercontent.com (uce5dd90142fbff04ab3f13be51c.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24450101 (23M) [text/plain]\n",
            "Saving to: ‘negative.csv’\n",
            "\n",
            "negative.csv        100%[===================>]  23.32M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-02-21 14:46:10 (201 MB/s) - ‘negative.csv’ saved [24450101/24450101]\n",
            "\n",
            "--2021-02-21 14:46:10--  https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.18, 2620:100:601c:18::a27d:612\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/fnpq3z4bcnoktiv/positive.csv [following]\n",
            "--2021-02-21 14:46:10--  https://www.dropbox.com/s/raw/fnpq3z4bcnoktiv/positive.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucb6758ceacad302d83a0635c1f9.dl.dropboxusercontent.com/cd/0/inline/BJUpUdbUxNTIzScQYqeyavE_q1nCggo-cxUoJHMjJXzFWrdpzSzMWoZWwtd1xo83CKSRIZwcZjcF68TH9j4y0_gDJXAEFpGZOmIaxaDxZUJ-Hw/file# [following]\n",
            "--2021-02-21 14:46:10--  https://ucb6758ceacad302d83a0635c1f9.dl.dropboxusercontent.com/cd/0/inline/BJUpUdbUxNTIzScQYqeyavE_q1nCggo-cxUoJHMjJXzFWrdpzSzMWoZWwtd1xo83CKSRIZwcZjcF68TH9j4y0_gDJXAEFpGZOmIaxaDxZUJ-Hw/file\n",
            "Resolving ucb6758ceacad302d83a0635c1f9.dl.dropboxusercontent.com (ucb6758ceacad302d83a0635c1f9.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:6019:15::a27d:40f\n",
            "Connecting to ucb6758ceacad302d83a0635c1f9.dl.dropboxusercontent.com (ucb6758ceacad302d83a0635c1f9.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26233379 (25M) [text/plain]\n",
            "Saving to: ‘positive.csv’\n",
            "\n",
            "positive.csv        100%[===================>]  25.02M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-02-21 14:46:11 (206 MB/s) - ‘positive.csv’ saved [26233379/26233379]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "_bsdMvYm-P4X",
        "outputId": "2f70bf61-6781-414b-b510-3292b5c8e639"
      },
      "source": [
        "cols = ['id', 'date', 'name', 'text', 'type', 'rep', 'rtw', 'fav', 'stcount', 'foll', 'frien', 'listcount'] \r\n",
        "\r\n",
        "df1 = pd.read_csv('positive.csv', delimiter=';', header=None, names=cols)\r\n",
        "df2 = pd.read_csv('negative.csv', delimiter=';', header=None, names=cols)\r\n",
        "\r\n",
        "df = pd.concat([df1,df2], ignore_index=True)\r\n",
        "df.sample(10)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>name</th>\n",
              "      <th>text</th>\n",
              "      <th>type</th>\n",
              "      <th>rep</th>\n",
              "      <th>rtw</th>\n",
              "      <th>fav</th>\n",
              "      <th>stcount</th>\n",
              "      <th>foll</th>\n",
              "      <th>frien</th>\n",
              "      <th>listcount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17884</th>\n",
              "      <td>409375369251147776</td>\n",
              "      <td>1386437668</td>\n",
              "      <td>Hermiiona</td>\n",
              "      <td>Ахаха, ходить в 23:30 в магазин, чтобы заплати...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>56</td>\n",
              "      <td>66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76415</th>\n",
              "      <td>410698319250210816</td>\n",
              "      <td>1386753084</td>\n",
              "      <td>pillSa5</td>\n",
              "      <td>США сознались, что на территории России имеют ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26465</td>\n",
              "      <td>417</td>\n",
              "      <td>320</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78997</th>\n",
              "      <td>410726836478681089</td>\n",
              "      <td>1386759883</td>\n",
              "      <td>MAMKA_XA33bI</td>\n",
              "      <td>В новом выпуске \"ВЗ\" с одной стороны 1D, с дру...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30670</td>\n",
              "      <td>4433</td>\n",
              "      <td>3486</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193919</th>\n",
              "      <td>419422148659539968</td>\n",
              "      <td>1388833007</td>\n",
              "      <td>deathbedz</td>\n",
              "      <td>Еще у меня в ленте в инстаграме было две дуры ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>18998</td>\n",
              "      <td>548</td>\n",
              "      <td>54</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103413</th>\n",
              "      <td>411111565061263360</td>\n",
              "      <td>1386851609</td>\n",
              "      <td>fudoko_senaemi</td>\n",
              "      <td>я два месяца пил витамины и в последний день п...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>45005</td>\n",
              "      <td>387</td>\n",
              "      <td>384</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11566</th>\n",
              "      <td>409301512393744384</td>\n",
              "      <td>1386420059</td>\n",
              "      <td>laura16_sl</td>\n",
              "      <td>А вот и мы) Аделина, Андрео, Самуэль и Валерио...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2880</td>\n",
              "      <td>149</td>\n",
              "      <td>97</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164668</th>\n",
              "      <td>415111875509374976</td>\n",
              "      <td>1387805358</td>\n",
              "      <td>zizizacuquka</td>\n",
              "      <td>Бесит, когда учителя добавляются в друзья! И о...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>45</td>\n",
              "      <td>15</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205947</th>\n",
              "      <td>421837982711046145</td>\n",
              "      <td>1389408987</td>\n",
              "      <td>pirogovaov</td>\n",
              "      <td>до сих пор болит не могу(((((ну всё щитай отра...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42339</th>\n",
              "      <td>409940971687014400</td>\n",
              "      <td>1386572518</td>\n",
              "      <td>SaintGutFree</td>\n",
              "      <td>Я лишь спрашиваю себя почему я еще не послала ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14156</td>\n",
              "      <td>76</td>\n",
              "      <td>31</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218556</th>\n",
              "      <td>423922666123579392</td>\n",
              "      <td>1389906014</td>\n",
              "      <td>dimasashurin</td>\n",
              "      <td>Я прям афиши вижу типо: СТИХИ КАБАНА. ЭКСКЛЮЗИ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4079</td>\n",
              "      <td>13</td>\n",
              "      <td>47</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        id        date            name  ...  foll  frien  listcount\n",
              "17884   409375369251147776  1386437668       Hermiiona  ...    56     66          0\n",
              "76415   410698319250210816  1386753084         pillSa5  ...   417    320         10\n",
              "78997   410726836478681089  1386759883    MAMKA_XA33bI  ...  4433   3486         30\n",
              "193919  419422148659539968  1388833007       deathbedz  ...   548     54          9\n",
              "103413  411111565061263360  1386851609  fudoko_senaemi  ...   387    384          6\n",
              "11566   409301512393744384  1386420059      laura16_sl  ...   149     97          1\n",
              "164668  415111875509374976  1387805358    zizizacuquka  ...    15     27          0\n",
              "205947  421837982711046145  1389408987      pirogovaov  ...     0      0          0\n",
              "42339   409940971687014400  1386572518    SaintGutFree  ...    76     31          3\n",
              "218556  423922666123579392  1389906014    dimasashurin  ...    13     47          0\n",
              "\n",
              "[10 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peHsLzmxKCqn"
      },
      "source": [
        "Проведем минимальную предобработку: уберем упоминания других пользователей, начинающихся с @ и соответствующую метку ретвита RT (то, что их очень много, можно увидеть в нескольких выведенных строках сверху). Также в текстах интернета часто могут быть какие-либо ссылки и е-мейлы, их также уберем (регулярки для емейлов и ссылок были найдены в интернете)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbtekjHJKSfg"
      },
      "source": [
        "def small_preprocessing(x):\r\n",
        "  x = re.sub(r'@[a-zA-Z_0-9]+', '', x)\r\n",
        "  x = re.sub(r'RT', '', x)\r\n",
        "  url_pattern = re.compile(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*')\r\n",
        "  email_pattern = re.compile(r'[\\w.-]+@[\\w.-]+.\\w+')\r\n",
        "  for pattern in (url_pattern, email_pattern):\r\n",
        "    x = pattern.sub('', x)\r\n",
        "  return x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpcUCMy-KlLS"
      },
      "source": [
        "Колонку с целевой переменной type преобразуем из значений -1 и 1 в привычные 0 и 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAg1bn1vijrY"
      },
      "source": [
        "df['type']= df['type']>0\r\n",
        "df['type'] = df['type'].astype(int)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALB8pxxSDw0e",
        "outputId": "e9d6ae0d-9237-4aac-f287-b4782084867a"
      },
      "source": [
        "df['type'].value_counts()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    114911\n",
              "0    111923\n",
              "Name: type, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pji6KsyaLbHM"
      },
      "source": [
        "Положительный и отрицательный классы сбалансированы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8uLQCDWLA-Q"
      },
      "source": [
        "texts = df['text'].apply(small_preprocessing)\r\n",
        "target = df['type'].values"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_JvXFcsKs0x"
      },
      "source": [
        "Делим данные на тренировочную и тестовую выборки (тестовая будет выполнять роль валидационной, на ней будем смотреть качество)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9LezfmwrYA1"
      },
      "source": [
        "train_data, test_data, train_labels, test_labels = train_test_split(texts, target, test_size=0.33, random_state=42, stratify=target)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WctHugKoDBj4"
      },
      "source": [
        "## 1. biLSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mf1vAWzO0Gq"
      },
      "source": [
        "### Обучаемый Embedding слой"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oir8BAiaK2m4"
      },
      "source": [
        "Преобразование токенов в идентификаторы можно делать не вручную, а через Tokinizer из keras. Он точно так же строит на корпусе текстов словарь word2id и маппит индексы к словами. Паддинги и oov слова также учитываются: для паддинга резервируется индекс 0, для oov - 1. Надо лишь отдельно указать, на какую последовательность символов заменять oov слова (на unk)\r\n",
        "\r\n",
        "\r\n",
        "Tokenizer также игнорирует знаки препинания и переводит текст в нижний регистр. Поэтому отдельную такую предобработку выше мы не делали. Лемматизацию проводить пока не будем, поскольку эмбеддинги обучаются самостоятельно, и, в принципе,  из-за лемматизации можно также потерять некоторые слова (какие-то особенные слова из твиттера могут не так лемматизироваться) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_wkWq07xwTm"
      },
      "source": [
        "Тokenizer фитится только на тренировочной выборке, поскольку в реальной жизни модель никогда заранее не узнает, какие будут тестовые данные, и как их надо специально подготовить."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXlhkAT7fC0i"
      },
      "source": [
        "tokenizer = Tokenizer(oov_token='unk')\r\n",
        "tokenizer.fit_on_texts(train_data)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txs4zWsNMotZ"
      },
      "source": [
        "Посмотрим сколько уникальных токенов имеется в нашем распоряжении"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzRjkJjRgDdI",
        "outputId": "0fecfa10-c0fe-4ce1-9371-02e07b0cc95b"
      },
      "source": [
        "len(tokenizer.word_counts)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "147627"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTvaJIqtziET"
      },
      "source": [
        "#выберем не очень большой размер эмбеддинга, который будет обучаться, чтобы не ждать долго обучения и не занимать много памяти\r\n",
        "embedding_size = 20\r\n",
        "word2index = tokenizer.word_index"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzOkUyZehlTI"
      },
      "source": [
        "tokenized_train = tokenizer.texts_to_sequences(train_data)\r\n",
        "tokenized_test = tokenizer.texts_to_sequences(test_data)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXPmU_3_fTVS",
        "outputId": "01f6c3df-ebd4-4b86-e11a-e97b2dc4e1e7"
      },
      "source": [
        "tokenized_train[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[24610,\n",
              " 60174,\n",
              " 24611,\n",
              " 60175,\n",
              " 3660,\n",
              " 60176,\n",
              " 24612,\n",
              " 2438,\n",
              " 39794,\n",
              " 1595,\n",
              " 73,\n",
              " 651,\n",
              " 18086,\n",
              " 2936,\n",
              " 1109]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjIOLcU-NP81"
      },
      "source": [
        "Добавим паддинги так, чтобы все последовательности имели длину, равную максимальной"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIWO5EoGfPgP",
        "outputId": "f7134132-aa4d-4d1b-a466-5cab77a6c04d"
      },
      "source": [
        "maxlen = max([len(x) for x in tokenized_train])\r\n",
        "print('Максимальная длина предложения: ', maxlen)\r\n",
        "\r\n",
        "padded_train = pad_sequences(tokenized_train, maxlen=maxlen)\r\n",
        "padded_test = pad_sequences(tokenized_test, maxlen=maxlen)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Максимальная длина предложения:  41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI5eVZNyiK2W",
        "outputId": "8ca54d75-9735-4cd1-c71b-618dac904494"
      },
      "source": [
        "padded_train[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0, 24610,\n",
              "       60174, 24611, 60175,  3660, 60176, 24612,  2438, 39794,  1595,\n",
              "          73,   651, 18086,  2936,  1109], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysGvU4CxNe1J"
      },
      "source": [
        "Собираем модель из Embedding слоя, biLSTM и полносвязного.\r\n",
        "\r\n",
        "(задание можно прочитать двусмысленно: после biLSTM один полносвязный слой, который является же выходным, либо два полносвязных, из которых один имеет произвольное число узлов, а другой - выходной. Я остановилась на первом варианте, потому что так было во всех семинарах и никакой разницы во времени/качестве между этими разными реализациями фактически нет, по крайней мере мне не удалось их обнаружить)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPaEuE-Xi8Dk",
        "outputId": "c2391027-47b3-4cbe-c7fa-011911ab3c37"
      },
      "source": [
        "input_layer = Input((maxlen,))\r\n",
        "embedding_layer = Embedding(input_dim=len(word2index), output_dim=embedding_size, mask_zero=True)(input_layer)\r\n",
        "lstm = Bidirectional(LSTM(128, return_sequences=False))(embedding_layer)\r\n",
        "output_layer = Dense(1, activation=\"sigmoid\")(lstm)\r\n",
        "\r\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\r\n",
        "model.compile(loss='binary_crossentropy',\r\n",
        "                  optimizer=Adam(learning_rate=0.001),\r\n",
        "                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\r\n",
        "print(model.summary())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 41)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 41, 20)            2952560   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 256)               152576    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 3,105,393\n",
            "Trainable params: 3,105,393\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A1Pmx1oOYDf"
      },
      "source": [
        "Не будем ставить большое количество эпох. Трех будет достаточно, чтобы посмотреть на то, как уменьшается ли ошибка и есть ли склонность к переобучению. Батч сайз подобрала на основе наблюдений и решенных ранее задач"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USb4Ye1_lECE",
        "outputId": "70c2b1e3-5ee6-43ce-c4ec-4f019007c37c"
      },
      "source": [
        "model.fit(padded_train, train_labels, \r\n",
        "          validation_data=(padded_test, test_labels), \r\n",
        "          batch_size=128, epochs = 3, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 151978 samples, validate on 74856 samples\n",
            "Epoch 1/3\n",
            "151978/151978 [==============================] - 134s 880us/step - loss: 0.5059 - accuracy: 0.7414 - precision: 0.6992 - recall: 0.7444 - val_loss: 0.4760 - val_accuracy: 0.7625 - val_precision: 0.7568 - val_recall: 0.7324\n",
            "Epoch 2/3\n",
            "151978/151978 [==============================] - 135s 886us/step - loss: 0.4026 - accuracy: 0.8122 - precision: 0.7807 - recall: 0.7443 - val_loss: 0.4822 - val_accuracy: 0.7678 - val_precision: 0.7896 - val_recall: 0.7535\n",
            "Epoch 3/3\n",
            "151978/151978 [==============================] - 133s 878us/step - loss: 0.3428 - accuracy: 0.8445 - precision: 0.8000 - recall: 0.7635 - val_loss: 0.5032 - val_accuracy: 0.7607 - val_precision: 0.8038 - val_recall: 0.7722\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f3048232eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmptnrFiRHX4"
      },
      "source": [
        "Лосс уменьшается достаточно быстро, показатели качества достаточно высокие и на обучающей и на валидационной выборке. НО: можно заметить, что значение лосса на валидационной выборке с каждой эпохой все увеличивается, а accuracy уменьшается. Это верный признак переобучения, что является огромным минусом это реализации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYb94nvOOwZ4"
      },
      "source": [
        "### Предобученные FastText эмбеддинги"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUlpb6XxmCVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7c947a9-600d-4243-c9d1-19b09ff364ed"
      },
      "source": [
        "!wget https://rusvectores.org/static/models/rusvectores4/fasttext/araneum_none_fasttextcbow_300_5_2018.tgz"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-21 18:37:34--  https://rusvectores.org/static/models/rusvectores4/fasttext/araneum_none_fasttextcbow_300_5_2018.tgz\n",
            "Resolving rusvectores.org (rusvectores.org)... 116.203.104.23\n",
            "Connecting to rusvectores.org (rusvectores.org)|116.203.104.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2691248108 (2.5G) [application/x-gzip]\n",
            "Saving to: ‘araneum_none_fasttextcbow_300_5_2018.tgz’\n",
            "\n",
            "araneum_none_fastte 100%[===================>]   2.51G  31.4MB/s    in 82s     \n",
            "\n",
            "2021-02-21 18:38:56 (31.5 MB/s) - ‘araneum_none_fasttextcbow_300_5_2018.tgz’ saved [2691248108/2691248108]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5gM6ZgatBs_"
      },
      "source": [
        "!tar -xf araneum_none_fasttextcbow_300_5_2018.tgz"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdIvJUkRtQB2"
      },
      "source": [
        "ft_file = 'araneum_none_fasttextcbow_300_5_2018.model'\r\n",
        "ft_vectors = KeyedVectors.load(ft_file)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNo2KnfVSA6e"
      },
      "source": [
        "Загрузили предобученную модель fasttext для русского языка. Для того, чтобы большему количеству слов сопоставить вектор из модели, тексты необходимо лемматизировать. Иначе только те слова, что по счастливому стечению обстоятельств имеют начальную форму в текстах, будут иметь качественное векторное представление."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Sw87_ZaE0ci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2474f7c8-cf76-46b6-9993-e3fbb453980d"
      },
      "source": [
        "%%time\r\n",
        "#ячейка работает довольно долго\r\n",
        "punkt = punctuation + '«»—…“”*№–'\r\n",
        "morph = pymorphy2.MorphAnalyzer()\r\n",
        "\r\n",
        "def lemmatizing(text):\r\n",
        "  clean_text = [word.strip(punkt) for word in text.lower().split()]\r\n",
        "  lem_text = ' '.join([morph.parse(word)[0].normal_form for word in clean_text])\r\n",
        "  return lem_text\r\n",
        "\r\n",
        "\r\n",
        "lem_train_data = train_data.apply(lemmatizing)\r\n",
        "lem_test_data = test_data.apply(lemmatizing)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 7min 44s, sys: 104 ms, total: 7min 45s\n",
            "Wall time: 7min 47s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPbkv8jEUH2T"
      },
      "source": [
        "Переводим тексты в последовательность идентификатором с помощью Tokenizer-а на лемматизированных данных. Добавляем паддинги. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEU8JGz4ncTn",
        "outputId": "aba99a86-e6b3-4656-efa4-66e1bc18ed9d"
      },
      "source": [
        "lem_tokenizer = Tokenizer(oov_token='unk')\r\n",
        "lem_tokenizer.fit_on_texts(lem_train_data)\r\n",
        "len(lem_tokenizer.word_counts)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98752"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNLEBGBwWG_4"
      },
      "source": [
        "Уникальных слов стало меньше благодаря лемматизации"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxLeERa9nroX"
      },
      "source": [
        "lem_tokenized_train = lem_tokenizer.texts_to_sequences(lem_train_data)\r\n",
        "lem_tokenized_test = lem_tokenizer.texts_to_sequences(lem_test_data)\r\n",
        "lem_padded_train = pad_sequences(lem_tokenized_train, maxlen=maxlen)\r\n",
        "lem_padded_test = pad_sequences(lem_tokenized_test, maxlen=maxlen)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8JWuGANtE0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "186fe5cd-aae9-444c-fbdf-ea81a42a3f18"
      },
      "source": [
        "#достаем из объекта Tokenizer словарь word2id\r\n",
        "lemma2index = lem_tokenizer.word_index\r\n",
        "#фиксируем размер эмбеддингов\r\n",
        "ft_embs_size = ft_vectors.vector_size\r\n",
        "#создаем матрицу размера (кол-во слов в корпусе; размер эмбеддинга). заполняем ее случайным образом из нормального распределения \r\n",
        "#(для этого необходимо посчитать среднее и корень из дисперсии всех векторов в fasttext модели)\r\n",
        "emb_mean, emb_std = ft_vectors.syn0.mean(), ft_vectors.syn0.std()\r\n",
        "embedding_matrix = np.random.normal(emb_mean, emb_std, (len(lemma2index)+1, ft_embs_size)) #единичка прибавляется, потому что индекса 0 в словаре word2index нет\r\n",
        "for word, i in lemma2index.items():\r\n",
        "    if word in ft_vectors:\r\n",
        "      embedding_vector = ft_vectors[word]\r\n",
        "      embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff2sCc4FEfVE",
        "outputId": "8301b579-7876-475e-b851-799eba06b8f9"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(98754, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jebLp8W2aXTa"
      },
      "source": [
        "Первая размерность на 2 больше, чем количество уникальных слов, указанных выше, из-за паддинга и UNK.\r\n",
        "\r\n",
        "Строим саму сеть (изменились лишь параметры в embedding слое)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1h-LRMqu1Fi",
        "outputId": "ec34132d-1523-49a4-b7e9-d9683dc086bd"
      },
      "source": [
        "input_layer = Input((maxlen,))\r\n",
        "embedding_layer = Embedding(input_dim=len(lemma2index)+1, output_dim=ft_embs_size, \r\n",
        "                            mask_zero=True, weights=[embedding_matrix],\r\n",
        "                            trainable=False)(input_layer)\r\n",
        "lstm = Bidirectional(LSTM(128, return_sequences=False))(embedding_layer)\r\n",
        "output_layer = Dense(1, activation=\"sigmoid\")(lstm)\r\n",
        "\r\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\r\n",
        "model.compile(loss='binary_crossentropy',\r\n",
        "                  optimizer=Adam(learning_rate=0.001),\r\n",
        "                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\r\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 41)                0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 41, 300)           29626200  \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 256)               439296    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 30,065,753\n",
            "Trainable params: 439,553\n",
            "Non-trainable params: 29,626,200\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c44cy_429Qi9",
        "outputId": "7e7c65e9-0f65-4685-eb7d-c6a370241371"
      },
      "source": [
        "model.fit(lem_padded_train, train_labels, \r\n",
        "          validation_data=(lem_padded_test, test_labels), \r\n",
        "          batch_size=128, epochs = 3, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 151978 samples, validate on 74856 samples\n",
            "Epoch 1/3\n",
            "151978/151978 [==============================] - 130s 857us/step - loss: 0.5539 - accuracy: 0.7041 - precision_1: 0.7058 - recall_1: 0.6505 - val_loss: 0.5361 - val_accuracy: 0.7184 - val_precision_1: 0.7196 - val_recall_1: 0.6898\n",
            "Epoch 2/3\n",
            "151978/151978 [==============================] - 127s 837us/step - loss: 0.5208 - accuracy: 0.7283 - precision_1: 0.7235 - recall_1: 0.7005 - val_loss: 0.5311 - val_accuracy: 0.7215 - val_precision_1: 0.7248 - val_recall_1: 0.7117\n",
            "Epoch 3/3\n",
            "151978/151978 [==============================] - 124s 816us/step - loss: 0.5044 - accuracy: 0.7403 - precision_1: 0.7258 - recall_1: 0.7196 - val_loss: 0.5095 - val_accuracy: 0.7383 - val_precision_1: 0.7309 - val_recall_1: 0.7205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f2fc7653278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfYM86_1kDv5"
      },
      "source": [
        "Loss убывает гораздо медленнее, чем в предыдущей сетке, качество, соответственно, тоже ниже. Однако надо обратить внимание на то, что у этой сети почти в три раза больше обучающихся параметров у слоя biLSTM (который по сути основной) - 439 тысяч vs 152 тысячи. Поэтому 3 эпохи - это достаточно мало, и не дает никаких оснований, чтобы говорить о том, что вторая сеть показывает худшие результаты, чем первая. Попробуем еще немного ее поучить: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLVRdm46cnqm",
        "outputId": "d73c8cbd-f13a-4498-eae8-4b9d7ad2872c"
      },
      "source": [
        "model.fit(lem_padded_train, train_labels, \r\n",
        "          validation_data=(lem_padded_test, test_labels), \r\n",
        "          batch_size=128, epochs = 4, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 151978 samples, validate on 74856 samples\n",
            "Epoch 1/4\n",
            "151978/151978 [==============================] - 127s 836us/step - loss: 0.4904 - accuracy: 0.7505 - precision_1: 0.7350 - recall_1: 0.7220 - val_loss: 0.5070 - val_accuracy: 0.7415 - val_precision_1: 0.7371 - val_recall_1: 0.7258\n",
            "Epoch 2/4\n",
            "151978/151978 [==============================] - 123s 812us/step - loss: 0.4781 - accuracy: 0.7596 - precision_1: 0.7393 - recall_1: 0.7297 - val_loss: 0.5026 - val_accuracy: 0.7419 - val_precision_1: 0.7422 - val_recall_1: 0.7303\n",
            "Epoch 3/4\n",
            "151978/151978 [==============================] - 125s 823us/step - loss: 0.4649 - accuracy: 0.7693 - precision_1: 0.7452 - recall_1: 0.7315 - val_loss: 0.5001 - val_accuracy: 0.7481 - val_precision_1: 0.7477 - val_recall_1: 0.7329\n",
            "Epoch 4/4\n",
            "151978/151978 [==============================] - 124s 814us/step - loss: 0.4508 - accuracy: 0.7789 - precision_1: 0.7503 - recall_1: 0.7347 - val_loss: 0.5032 - val_accuracy: 0.7495 - val_precision_1: 0.7522 - val_recall_1: 0.7369\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f2fba7e6f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHWo5tdrcI-f"
      },
      "source": [
        "Качество с каждой эпохой повышается и на обучающей и на валидационной выборке, лосс также равномерно убывает. Это говорит о том, что сеть продолжает обучаться. Попробуем пройти еще через 4 эпохи:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kcag7lTGh9H3",
        "outputId": "07495a3f-8a13-496a-8ab5-42e327b42f46"
      },
      "source": [
        "model.fit(lem_padded_train, train_labels, \r\n",
        "          validation_data=(lem_padded_test, test_labels), \r\n",
        "          batch_size=128, epochs = 4, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 151978 samples, validate on 74856 samples\n",
            "Epoch 1/4\n",
            "151978/151978 [==============================] - 126s 831us/step - loss: 0.4355 - accuracy: 0.7886 - precision_1: 0.7543 - recall_1: 0.7392 - val_loss: 0.5053 - val_accuracy: 0.7499 - val_precision_1: 0.7562 - val_recall_1: 0.7412\n",
            "Epoch 2/4\n",
            "151978/151978 [==============================] - 126s 829us/step - loss: 0.4180 - accuracy: 0.7991 - precision_1: 0.7583 - recall_1: 0.7432 - val_loss: 0.5227 - val_accuracy: 0.7466 - val_precision_1: 0.7605 - val_recall_1: 0.7442\n",
            "Epoch 3/4\n",
            "151978/151978 [==============================] - 125s 823us/step - loss: 0.3985 - accuracy: 0.8100 - precision_1: 0.7630 - recall_1: 0.7455 - val_loss: 0.5263 - val_accuracy: 0.7436 - val_precision_1: 0.7645 - val_recall_1: 0.7477\n",
            "Epoch 4/4\n",
            "151978/151978 [==============================] - 126s 828us/step - loss: 0.3776 - accuracy: 0.8237 - precision_1: 0.7663 - recall_1: 0.7502 - val_loss: 0.5503 - val_accuracy: 0.7436 - val_precision_1: 0.7683 - val_recall_1: 0.7516\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f2fba7d7860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR1rDM9_ivdd"
      },
      "source": [
        "На этом, пожалуй, стоит остановиться, потому что качество на валидационной выборке перестало расти, и валидационный лосс только повышается - признаки переобучения. Показатели качества достигли хорошей отметки: на тренировочной выборке accuracy почти догнал результат предыдущей сети (82 и 84). Точность и полнота немного ниже (точность на 4 процента, полнота на 1). Все значения метрик на валидационной выборке находятся в районе 75 процентов, что тоже на 2-3 процента ниже результата, полученной предыдущей сетью. Такая разница все же довольно большая, кроме того дополнительно тратится время и ресурсы на лемматизацию и загрузку модели.\r\n",
        "\r\n",
        "Но огромным преимуществом является отсутствие такого сильного переобучения, как в первом случае, где лосс на валидационной выборке сразу же начал расти. Здесь он плавно убывает, и только на 7-8 эпохах начинает стоять на одном месте, а затем расти"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9G8pARoPHZ3"
      },
      "source": [
        "### Обучаемые FastText эмбеддинги"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4bT3gID0Ycr"
      },
      "source": [
        "Обучим с нуля Fasttext эмбеддинги на лемматизированной тренировочной выборке. Тренировочной - исходя из соображений, что мы не должны, по идее, заранее видеть тестовые данные. Лемматизированной - потому что уникальных слов в ней меньше, удалены знаки препинания, соответственно, модель будет немного легче. Если обучать на неподготовленных данных, то велик шанс, что в колабе кончится ОЗУ\r\n",
        "\r\n",
        "Размер эмбеддинга тоже подбирался исходя из того, на сколько хватит памяти колаба (200 уже вылетает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v0-mQbc-DTs"
      },
      "source": [
        "new_ft = FastText(lem_train_data, size=150, iter=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An4nIMWn1J2n"
      },
      "source": [
        "Точно так же, как и с предобученными эмбеддингами, создаем матрицу размера (размер словаря; размер вектора)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74MFSE_qzpGr",
        "outputId": "7ee0d90d-1cb6-4872-8dc3-60442013735e"
      },
      "source": [
        "emb_mean, emb_std = new_ft.wv.syn0.mean(), new_ft.wv.syn0.std()\r\n",
        "\r\n",
        "ft_embs_size = new_ft.vector_size\r\n",
        "new_embedding_matrix = np.random.normal(emb_mean, emb_std, (len(lemma2index)+1, ft_embs_size))\r\n",
        "for word, i in lemma2index.items():\r\n",
        "    if word in new_ft:\r\n",
        "      embedding_vector = new_ft[word]\r\n",
        "      new_embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZe1I1PG1eTe"
      },
      "source": [
        "Строим точно такую же сетку:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToRxSese0tSd",
        "outputId": "c85e5f45-2e19-4c55-9791-bf8362d31e8b"
      },
      "source": [
        "input_layer = Input((maxlen,))\r\n",
        "embedding_layer = Embedding(input_dim=len(lemma2index)+1, output_dim=ft_embs_size, \r\n",
        "                            mask_zero=True, weights=[new_embedding_matrix],\r\n",
        "                            trainable=False)(input_layer)\r\n",
        "lstm = Bidirectional(LSTM(128, return_sequences=False))(embedding_layer)\r\n",
        "output_layer = Dense(1, activation=\"sigmoid\")(lstm)\r\n",
        "\r\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\r\n",
        "model.compile(loss='binary_crossentropy',\r\n",
        "                  optimizer=Adam(learning_rate=0.001),\r\n",
        "                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\r\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 41)                0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 41, 150)           14813100  \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 256)               285696    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 15,099,053\n",
            "Trainable params: 285,953\n",
            "Non-trainable params: 14,813,100\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiNd3qGM00iv",
        "outputId": "e89becea-0849-45e0-89d4-184dd117f4e4"
      },
      "source": [
        "model.fit(lem_padded_train, train_labels, \r\n",
        "          validation_data=(lem_padded_test, test_labels), \r\n",
        "          batch_size=128, epochs = 3, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 151978 samples, validate on 74856 samples\n",
            "Epoch 1/3\n",
            "151978/151978 [==============================] - 124s 818us/step - loss: 0.6310 - accuracy: 0.5904 - precision_2: 0.6022 - recall_2: 0.5368 - val_loss: 0.6298 - val_accuracy: 0.5978 - val_precision_2: 0.6156 - val_recall_2: 0.5168\n",
            "Epoch 2/3\n",
            "151978/151978 [==============================] - 122s 805us/step - loss: 0.6273 - accuracy: 0.5976 - precision_2: 0.6191 - recall_2: 0.5152 - val_loss: 0.6278 - val_accuracy: 0.6004 - val_precision_2: 0.6206 - val_recall_2: 0.5172\n",
            "Epoch 3/3\n",
            "151978/151978 [==============================] - 122s 805us/step - loss: 0.6254 - accuracy: 0.6015 - precision_2: 0.6222 - recall_2: 0.5171 - val_loss: 0.6270 - val_accuracy: 0.6003 - val_precision_2: 0.6242 - val_recall_2: 0.5153\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f17b00ccef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIiwYSun1jTJ"
      },
      "source": [
        "3 эпохи опять же достаточно мало для этой сети. Но можно заметить, что лосс уменьшается очень медленно, и качество пока что достаточно низкое, по сравнению с предыдущими двумя реализациями, особенно низкая полнота. Попробуем пообучать еще 5 эпох"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzsQ52-p3Tet",
        "outputId": "df75c790-ddc7-439a-b7ae-b444e6289fd6"
      },
      "source": [
        "model.fit(lem_padded_train, train_labels, \r\n",
        "          validation_data=(lem_padded_test, test_labels), \r\n",
        "          batch_size=128, epochs = 5, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 151978 samples, validate on 74856 samples\n",
            "Epoch 1/5\n",
            "151978/151978 [==============================] - 122s 801us/step - loss: 0.6241 - accuracy: 0.6025 - precision_2: 0.6257 - recall_2: 0.5129 - val_loss: 0.6265 - val_accuracy: 0.6011 - val_precision_2: 0.6270 - val_recall_2: 0.5122\n",
            "Epoch 2/5\n",
            "151978/151978 [==============================] - 122s 800us/step - loss: 0.6231 - accuracy: 0.6037 - precision_2: 0.6279 - recall_2: 0.5118 - val_loss: 0.6254 - val_accuracy: 0.6045 - val_precision_2: 0.6285 - val_recall_2: 0.5124\n",
            "Epoch 3/5\n",
            "151978/151978 [==============================] - 122s 801us/step - loss: 0.6218 - accuracy: 0.6060 - precision_2: 0.6289 - recall_2: 0.5135 - val_loss: 0.6281 - val_accuracy: 0.5952 - val_precision_2: 0.6302 - val_recall_2: 0.5106\n",
            "Epoch 4/5\n",
            "151978/151978 [==============================] - 121s 794us/step - loss: 0.6205 - accuracy: 0.6089 - precision_2: 0.6315 - recall_2: 0.5080 - val_loss: 0.6260 - val_accuracy: 0.6039 - val_precision_2: 0.6323 - val_recall_2: 0.5080\n",
            "Epoch 5/5\n",
            "151978/151978 [==============================] - 121s 793us/step - loss: 0.6190 - accuracy: 0.6107 - precision_2: 0.6328 - recall_2: 0.5087 - val_loss: 0.6242 - val_accuracy: 0.6050 - val_precision_2: 0.6330 - val_recall_2: 0.5104\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f17b00f0c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf_LItyC2YrF"
      },
      "source": [
        "Тенденции не поменялись: лосс на обеих выборках убывает очень медленно, качество низкое, хотя и повышается на несколько сотых после каждой эпохи. Это говорит о том, что нашими весами не получается достаточно хорошо минимизировать функцию ошибки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "excKvVIx3Q_e"
      },
      "source": [
        "Попробуем обучить эмбеддинги Fasttext с нуля на нелемматизированных данных: вдруг результат будет лучше. Тогда все равно нужно избавиться от пунктуации и верхнего регистра:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTd-NIUK9Yi_"
      },
      "source": [
        "punkt = punctuation + '«»—…“”*№–'\r\n",
        "\r\n",
        "def preprocess(text):\r\n",
        "  clean_text = ' '.join([word.strip(punkt) for word in text.lower().split()])\r\n",
        "  return clean_text\r\n",
        "\r\n",
        "clean_train_data = train_data.apply(preprocess)\r\n",
        "clean_test_data = test_data.apply(preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN1cM9ER3kni"
      },
      "source": [
        "Обучаем эмбеддинги с теми же параметрами и создаем матрицу весов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBgDe4W-9-3m"
      },
      "source": [
        "new_ft = FastText(clean_train_data, size=150, iter=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eee5Swo9-L9K",
        "outputId": "fe15e94a-82b2-44b2-e1a5-4404afff3f40"
      },
      "source": [
        "emb_mean, emb_std = new_ft.wv.syn0.mean(), new_ft.wv.syn0.std()\r\n",
        "\r\n",
        "ft_embs_size = new_ft.vector_size\r\n",
        "new_embedding_matrix = np.random.normal(emb_mean, emb_std, (len(word2index)+1, ft_embs_size))\r\n",
        "for word, i in word2index.items():\r\n",
        "    if word in new_ft:\r\n",
        "      embedding_vector = new_ft[word]\r\n",
        "      new_embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS1EJ-Ub3tlr"
      },
      "source": [
        "Формируем сеть и обучаем ее так же 8 эпох"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKVYqiui-dVU",
        "outputId": "508e52ea-94fe-40e6-9b61-f3eafba63fc9"
      },
      "source": [
        "input_layer = Input((maxlen,))\r\n",
        "embedding_layer = Embedding(input_dim=len(word2index)+1, output_dim=ft_embs_size, \r\n",
        "                            mask_zero=True, weights=[new_embedding_matrix],\r\n",
        "                            trainable=False)(input_layer)\r\n",
        "lstm = Bidirectional(LSTM(128, return_sequences=False))(embedding_layer)\r\n",
        "output_layer = Dense(1, activation=\"sigmoid\")(lstm)\r\n",
        "\r\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\r\n",
        "model.compile(loss='binary_crossentropy',\r\n",
        "                  optimizer=Adam(learning_rate=0.001),\r\n",
        "                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\r\n",
        "\r\n",
        "\r\n",
        "model.fit(padded_train, train_labels, \r\n",
        "          validation_data=(padded_test, test_labels), \r\n",
        "          batch_size=128, epochs = 8, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 151978 samples, validate on 74856 samples\n",
            "Epoch 1/8\n",
            "151978/151978 [==============================] - 122s 803us/step - loss: 0.6336 - accuracy: 0.5862 - precision_1: 0.6168 - recall_1: 0.4562 - val_loss: 0.6325 - val_accuracy: 0.5891 - val_precision_1: 0.6236 - val_recall_1: 0.4638\n",
            "Epoch 2/8\n",
            "151978/151978 [==============================] - 121s 797us/step - loss: 0.6296 - accuracy: 0.5949 - precision_1: 0.6222 - recall_1: 0.4803 - val_loss: 0.6297 - val_accuracy: 0.5946 - val_precision_1: 0.6265 - val_recall_1: 0.4752\n",
            "Epoch 3/8\n",
            "151978/151978 [==============================] - 120s 790us/step - loss: 0.6272 - accuracy: 0.5975 - precision_1: 0.6290 - recall_1: 0.4736 - val_loss: 0.6283 - val_accuracy: 0.5984 - val_precision_1: 0.6300 - val_recall_1: 0.4762\n",
            "Epoch 4/8\n",
            "151978/151978 [==============================] - 120s 788us/step - loss: 0.6254 - accuracy: 0.6010 - precision_1: 0.6307 - recall_1: 0.4789 - val_loss: 0.6272 - val_accuracy: 0.5996 - val_precision_1: 0.6337 - val_recall_1: 0.4746\n",
            "Epoch 5/8\n",
            "151978/151978 [==============================] - 118s 779us/step - loss: 0.6237 - accuracy: 0.6048 - precision_1: 0.6364 - recall_1: 0.4712 - val_loss: 0.6251 - val_accuracy: 0.6043 - val_precision_1: 0.6387 - val_recall_1: 0.4694\n",
            "Epoch 6/8\n",
            "151978/151978 [==============================] - 118s 775us/step - loss: 0.6204 - accuracy: 0.6123 - precision_1: 0.6411 - recall_1: 0.4686 - val_loss: 0.6194 - val_accuracy: 0.6194 - val_precision_1: 0.6420 - val_recall_1: 0.4728\n",
            "Epoch 7/8\n",
            "151978/151978 [==============================] - 118s 773us/step - loss: 0.6135 - accuracy: 0.6273 - precision_1: 0.6431 - recall_1: 0.4784 - val_loss: 0.6121 - val_accuracy: 0.6338 - val_precision_1: 0.6443 - val_recall_1: 0.4848\n",
            "Epoch 8/8\n",
            "151978/151978 [==============================] - 117s 772us/step - loss: 0.6064 - accuracy: 0.6391 - precision_1: 0.6454 - recall_1: 0.4919 - val_loss: 0.6104 - val_accuracy: 0.6361 - val_precision_1: 0.6460 - val_recall_1: 0.4996\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7feef7a98710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhSYmHht32v0"
      },
      "source": [
        "Различия действительно есть: значение лосса опустилось немного ниже, чем в варианте с лемматизированными данными. Немного выше значения accuracy и точности и на тренировочной и на тестовой выборках. Зато полнота очень низкая. Классификатор предсказывает верно малое количество предложений, относящихся к положительному классу.\r\n",
        "\r\n",
        "Разница в результатах, полученными ранее, составляет более 10 процентов по всем метрикам - это очень большое значение."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sywT7ArbFdK2"
      },
      "source": [
        "### Снова обучаемый Embedding слой"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JqkJhKU5nyv"
      },
      "source": [
        "Высокие показатели метрик были получены с помощью обучения эмбеддингов внутри модели. Этот вариант оказался также самым быстрым (предобученные эмбеддинги не сильно отстали по качеству, но зато значительно по подготовке и по времени обучения). \r\n",
        "\r\n",
        "\r\n",
        "Для справедливости нужно попробовать реализовать этот вариант еще и  на лемматизированных данных, вдруг он покажет себя очень плохо или, наоборот, еще лучше. Также увеличим размер эмбеддинга, чтобы можно было сопоставить с fasttext. \r\n",
        "Добавим в архитектуру дропаут, чтобы минимизировать переобучение, поскольку модель становится уже тяжелой"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqXi3-BWD3lq",
        "outputId": "bfacb7f2-f7a2-47db-b47e-1f4a84704a22"
      },
      "source": [
        "input_layer = Input((maxlen,))\r\n",
        "embedding_layer = Embedding(input_dim=len(lemma2index)+1, output_dim=150, mask_zero=True)(input_layer)\r\n",
        "lstm = Bidirectional(LSTM(128, return_sequences=False))(embedding_layer)\r\n",
        "d = Dropout(0.2)(lstm)\r\n",
        "output_layer = Dense(1, activation=\"sigmoid\")(d)\r\n",
        "\r\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\r\n",
        "model.compile(loss='binary_crossentropy',\r\n",
        "                  optimizer=Adam(learning_rate=0.001),\r\n",
        "                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\r\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 41)                0         \n",
            "_________________________________________________________________\n",
            "embedding_4 (Embedding)      (None, 41, 150)           14813100  \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 256)               285696    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 15,099,053\n",
            "Trainable params: 15,099,053\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fjq3CrSKEmym",
        "outputId": "29dd8dd5-6a26-4ba9-ee98-12dd50ed75a3"
      },
      "source": [
        "model.fit(lem_padded_train, train_labels, \r\n",
        "          validation_data=(lem_padded_test, test_labels), \r\n",
        "          batch_size=128, epochs = 3, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 151978 samples, validate on 74856 samples\n",
            "Epoch 1/3\n",
            "151978/151978 [==============================] - 134s 881us/step - loss: 0.4976 - accuracy: 0.7478 - precision_3: 0.7176 - recall_3: 0.7418 - val_loss: 0.4648 - val_accuracy: 0.7721 - val_precision_3: 0.7607 - val_recall_3: 0.7456\n",
            "Epoch 2/3\n",
            "151978/151978 [==============================] - 133s 875us/step - loss: 0.3552 - accuracy: 0.8404 - precision_3: 0.7867 - recall_3: 0.7663 - val_loss: 0.4882 - val_accuracy: 0.7663 - val_precision_3: 0.7974 - val_recall_3: 0.7785\n",
            "Epoch 3/3\n",
            "151978/151978 [==============================] - 133s 872us/step - loss: 0.2342 - accuracy: 0.9003 - precision_3: 0.8111 - recall_3: 0.7940 - val_loss: 0.5698 - val_accuracy: 0.7576 - val_precision_3: 0.8193 - val_recall_3: 0.8049\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7feeae5f60f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E08EZs-q6jT9"
      },
      "source": [
        "Всего за три эпохи лосс достиг маленького значения, аccuracy увеличилось до 90 на, точность и полнота примерно до 80 на тренировочной выборке, а на тестовой наблюдаем высокие значения точности и полноты (эти результаты выше, чем на сетке с нелемматизированными данными), но зато не очень большое accuracy (примерно такое же, как и в первом варианте)\r\n",
        "\r\n",
        "Loss на валидационной выборке так же растет, как и с меньшей размерностью эмбеддинга. Это большой минус у этой реализации. Мне не удалось с помощью добавления дропаута и регуляризации заставить ошибку на валидационной выборке убывать, поэтому во второй части задания я буду использовать предобученные эмбеддинги. Может быть, будет проигрыш в несколько процентов в качестве и во времени подготовки данных, но зато модель будет более стабильна, а это очень важно"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sAUEnhCDNea"
      },
      "source": [
        "## 2. biLSTM + charCNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vggk9qDmDWX2"
      },
      "source": [
        "Создадим отфильтрованный список лемм: в него войдут только те, что встречались в корпусе более пяти раз. Таким образом мы отсеем редкие, незначительные символы и какой-то лишний мусор"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg5MIovVEqar"
      },
      "source": [
        "filtered_vocab = []\r\n",
        "\r\n",
        "for k,v in lem_tokenizer.word_counts.items():\r\n",
        "  if v > 5:\r\n",
        "    filtered_vocab.append(k)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY5DjhUgH_R6",
        "outputId": "5534bef9-fd33-4345-dd0e-5786ef78e65e"
      },
      "source": [
        "chars = set([letter for word in filtered_vocab for letter in word])\r\n",
        "print(chars)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'ғ', 'т', 'ф', 'e', 'щ', '0', 'a', 'в', 'э', 'h', 'х', '1', '•', 'б', 'n', '3', 'ў', 'u', 'з', 'ь', 'f', 'ъ', 'й', 'о', '❤', '❄', '4', 'q', 'b', '2', 'ї', 'ч', '6', 'd', 'ж', 'и', '☆', '7', 'с', 'v', 'p', 'қ', 'ы', '©', 'j', 'у', 'ө', '♥', 'є', 'е', '°', 'к', 'п', '✌', 'ц', 'ё', 'і', '8', 'l', 'я', 'д', 'н', '9', '✔', 'i', 'r', 'ү', '️', 'ë', 'y', 'w', 'm', '☺', 'м', 'c', 'x', 's', 'k', 'g', '5', 'а', 'ә', 'ю', 'ң', 'л', '♡', 't', 'o', 'ш', 'z', 'г', 'р'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAxEB-TrDo_m"
      },
      "source": [
        "Помимо русских и английских букв присутствуют эмодзи, цифры и буквы из каких-то других алфавитов. Встречаемость даже странных символов очень высокая. \r\n",
        "\r\n",
        "Создадим словарь char2id и определим максимальную длину слова, чтобы дополнять паддингами все слова до этого значения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmZWsTy3GZXS",
        "outputId": "c18d8a56-c51d-4337-f198-a6084480e50f"
      },
      "source": [
        "char2id = {c: i + 2 for i, c in enumerate(chars)}\r\n",
        "char2id[\"pad\"] = 0\r\n",
        "char2id[\"unk\"] = 1\r\n",
        "\r\n",
        "char_maxlen = max(len(x) for x in filtered_vocab)\r\n",
        "print(\"Максимальная длина слова:\", char_maxlen)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Максимальная длина слова: 33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apd65yx-EO8K"
      },
      "source": [
        "Функция из семинара, которая трансформирует корпус текстов в трехмерный массив, где каждый обучающий пример представлен набором идентификаторов токенов, а каждый токен набором идентификаторов символов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaSSFbAlGhRd"
      },
      "source": [
        "def make_X_char(sentences):\r\n",
        "  X_char = []\r\n",
        "  for sentence in sentences:\r\n",
        "      sent_seq = []\r\n",
        "      for i in range(maxlen):\r\n",
        "          word_seq = []\r\n",
        "          for j in range(char_maxlen):\r\n",
        "              try:\r\n",
        "                  word_seq.append(char2id[sentence[i][j].lower()])\r\n",
        "              except:\r\n",
        "                  word_seq.append(char2id[\"pad\"])\r\n",
        "          sent_seq.append(word_seq)\r\n",
        "      X_char.append(np.array(sent_seq))\r\n",
        "  return np.array(X_char)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBjfy8bpEjWV"
      },
      "source": [
        "Преобразуем в этот формат лемматизированные обучающую и тестовую выборки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFHQxD4TJUt4"
      },
      "source": [
        "char_train, char_test = make_X_char(lem_train_data), make_X_char(lem_test_data)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzXl973eLdXw",
        "outputId": "9e405047-4041-46a2-f93b-6c4a002132fe"
      },
      "source": [
        "char_train.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(151978, 41, 33)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkyMTcehEo6I"
      },
      "source": [
        "Строим саму сеть с двумя входами, сверточным слоем и двумя двусторонними LSTM: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIJPPlFfMDno",
        "outputId": "71334e7c-0f41-463f-84e1-46843a5b08db"
      },
      "source": [
        "word_in = Input((maxlen,))\r\n",
        "emb_word = Embedding(input_dim=len(lemma2index)+1, output_dim=ft_embs_size, \r\n",
        "                     mask_zero=True, weights=[embedding_matrix], trainable=False)(word_in)\r\n",
        "\r\n",
        "f_lstm = Bidirectional(LSTM(units=64, return_sequences=False, recurrent_dropout=0.3))(emb_word)\r\n",
        "\r\n",
        "char_in = Input((maxlen, char_maxlen,))\r\n",
        "emb_char = TimeDistributed(Embedding(input_dim=len(char2id), output_dim=10, input_length=char_maxlen))(char_in)\r\n",
        "char_enc = TimeDistributed(Conv1D(filters=12, kernel_size=3))(emb_char)\r\n",
        "char_flat = TimeDistributed(Flatten())(char_enc)\r\n",
        "s_lstm = Bidirectional(LSTM(units=64, return_sequences=False, recurrent_dropout=0.3))(char_flat)\r\n",
        "\r\n",
        "x = concatenate([f_lstm, s_lstm])\r\n",
        "out = Dense(1, activation=\"sigmoid\")(x)\r\n",
        "\r\n",
        "model = Model(inputs=[char_in, word_in], outputs=out)\r\n",
        "\r\n",
        "\r\n",
        "model.compile(loss='binary_crossentropy',\r\n",
        "              optimizer=Adam(learning_rate=0.001),\r\n",
        "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\r\n",
        "\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_12 (InputLayer)           (None, 41, 33)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_16 (TimeDistri (None, 41, 33, 10)   940         input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_11 (InputLayer)           (None, 41)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_17 (TimeDistri (None, 41, 31, 12)   372         time_distributed_16[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "embedding_11 (Embedding)        (None, 41, 300)      29626200    input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_18 (TimeDistri (None, 41, 372)      0           time_distributed_17[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_11 (Bidirectional (None, 128)          186880      embedding_11[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_12 (Bidirectional (None, 128)          223744      time_distributed_18[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 256)          0           bidirectional_11[0][0]           \n",
            "                                                                 bidirectional_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1)            257         concatenate_6[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 30,038,393\n",
            "Trainable params: 412,193\n",
            "Non-trainable params: 29,626,200\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CE41bxaAS2xl",
        "outputId": "72e73572-8b4a-45b9-eeb5-67cd5854020d"
      },
      "source": [
        "model.fit([char_train, lem_padded_train], train_labels, \r\n",
        "          validation_data=([char_test, lem_padded_test], test_labels), \r\n",
        "          batch_size=128, epochs = 5, verbose = 1)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 151978 samples, validate on 74856 samples\n",
            "Epoch 1/5\n",
            "151978/151978 [==============================] - 221s 1ms/step - loss: 0.5596 - accuracy: 0.6998 - precision_4: 0.6865 - recall_4: 0.6576 - val_loss: 0.5394 - val_accuracy: 0.7193 - val_precision_4: 0.7183 - val_recall_4: 0.6816\n",
            "Epoch 2/5\n",
            "151978/151978 [==============================] - 216s 1ms/step - loss: 0.5276 - accuracy: 0.7233 - precision_4: 0.7243 - recall_4: 0.6900 - val_loss: 0.5270 - val_accuracy: 0.7257 - val_precision_4: 0.7264 - val_recall_4: 0.6989\n",
            "Epoch 3/5\n",
            "151978/151978 [==============================] - 215s 1ms/step - loss: 0.5145 - accuracy: 0.7320 - precision_4: 0.7280 - recall_4: 0.7058 - val_loss: 0.5121 - val_accuracy: 0.7364 - val_precision_4: 0.7328 - val_recall_4: 0.7050\n",
            "Epoch 4/5\n",
            "151978/151978 [==============================] - 213s 1ms/step - loss: 0.5053 - accuracy: 0.7398 - precision_4: 0.7367 - recall_4: 0.7052 - val_loss: 0.5078 - val_accuracy: 0.7398 - val_precision_4: 0.7380 - val_recall_4: 0.7095\n",
            "Epoch 5/5\n",
            "151978/151978 [==============================] - 212s 1ms/step - loss: 0.4966 - accuracy: 0.7468 - precision_4: 0.7395 - recall_4: 0.7132 - val_loss: 0.5081 - val_accuracy: 0.7390 - val_precision_4: 0.7401 - val_recall_4: 0.7172\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fbe5362e320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwwl34hsEy8-"
      },
      "source": [
        "Лосс на обеих выборках уменьшается, причем не так медленно, как это было с fasttext эмбеддингами на biLSTM. Значения метрик умеренно высокие: accuracy и точность около 74 на обеих выборках, и полнота 71. Если в точности и полноте мы роиграли модели с обучаемыми в процессе эмбеддингами, то accuracy на валидационной выборке у нее был примерно тот же. Зато, как я писала выше, в этом варианте реализации отсутствует такое сильное переобучение (есть только признаки его начала на 4 эпохе)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itKxg86wF5eu"
      },
      "source": [
        "Усложним архитектуру, чтобы попробовать получить более высокое качество. Я удвоила количество юнитов в обоих LSTM слоях, количество фильтров на сверточном слое, после него добавила дропаут.\r\n",
        "После слоя конкатенации добавила полносвязный слой. Основная мотивация в том, чтобы усложить сетку , чтобы она смогла выявить более сложные зависимости в данных, но и избежать переобучения с помощью дропаута. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVbqqVd-Vj1s",
        "outputId": "28322c08-09b3-44cd-c476-04ccd622a3db"
      },
      "source": [
        "word_in = Input((maxlen,))\r\n",
        "emb_word = Embedding(input_dim=len(lemma2index)+1, output_dim=ft_embs_size, \r\n",
        "                     mask_zero=True, weights=[embedding_matrix], trainable=False)(word_in)\r\n",
        "f_lstm = Bidirectional(LSTM(units=128, return_sequences=False, recurrent_dropout=0.3))(emb_word)\r\n",
        "\r\n",
        "char_in = Input((maxlen, char_maxlen,))\r\n",
        "emb_char = TimeDistributed(Embedding(input_dim=len(char2id), output_dim=10, input_length=char_maxlen))(char_in)\r\n",
        "char_enc = TimeDistributed(Conv1D(filters=24, kernel_size=3))(emb_char)\r\n",
        "char_enc = Dropout(0.5)(char_enc)\r\n",
        "char_flat = TimeDistributed(Flatten())(char_enc)\r\n",
        "s_lstm = Bidirectional(LSTM(units=128, return_sequences=False, recurrent_dropout=0.3))(char_flat)\r\n",
        "\r\n",
        "x = concatenate([f_lstm, s_lstm])\r\n",
        "x = Dense(64, activation='relu')(x)\r\n",
        "out = Dense(1, activation=\"sigmoid\")(x)\r\n",
        "\r\n",
        "model = Model(inputs=[char_in, word_in], outputs=out)\r\n",
        "\r\n",
        "\r\n",
        "model.compile(loss='binary_crossentropy',\r\n",
        "              optimizer=Adam(learning_rate=0.001),\r\n",
        "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\r\n",
        "\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_20 (InputLayer)           (None, 41, 33)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_28 (TimeDistri (None, 41, 33, 10)   940         input_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_29 (TimeDistri (None, 41, 31, 24)   744         time_distributed_28[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "input_19 (InputLayer)           (None, 41)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 41, 31, 24)   0           time_distributed_29[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "embedding_19 (Embedding)        (None, 41, 300)      29626200    input_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_30 (TimeDistri (None, 41, 744)      0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_19 (Bidirectional (None, 256)          439296      embedding_19[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_20 (Bidirectional (None, 256)          893952      time_distributed_30[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 512)          0           bidirectional_19[0][0]           \n",
            "                                                                 bidirectional_20[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 64)           32832       concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 1)            65          dense_13[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 30,994,029\n",
            "Trainable params: 1,367,829\n",
            "Non-trainable params: 29,626,200\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUxGB-eDWUtB",
        "outputId": "a34b6cbe-e151-4795-8907-d6e93722f9d7"
      },
      "source": [
        "model.fit([char_train, lem_padded_train], train_labels, \r\n",
        "          validation_data=([char_test, lem_padded_test], test_labels), \r\n",
        "          batch_size=128, epochs = 5, verbose = 1)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 151978 samples, validate on 74856 samples\n",
            "Epoch 1/5\n",
            "151978/151978 [==============================] - 227s 1ms/step - loss: 0.5512 - accuracy: 0.7047 - precision_7: 0.6985 - recall_7: 0.6500 - val_loss: 0.5299 - val_accuracy: 0.7231 - val_precision_7: 0.7213 - val_recall_7: 0.6903\n",
            "Epoch 2/5\n",
            "151978/151978 [==============================] - 222s 1ms/step - loss: 0.5221 - accuracy: 0.7268 - precision_7: 0.7255 - recall_7: 0.7011 - val_loss: 0.5223 - val_accuracy: 0.7287 - val_precision_7: 0.7270 - val_recall_7: 0.7103\n",
            "Epoch 3/5\n",
            "151978/151978 [==============================] - 220s 1ms/step - loss: 0.5095 - accuracy: 0.7356 - precision_7: 0.7287 - recall_7: 0.7167 - val_loss: 0.5215 - val_accuracy: 0.7286 - val_precision_7: 0.7340 - val_recall_7: 0.7125\n",
            "Epoch 4/5\n",
            "151978/151978 [==============================] - 219s 1ms/step - loss: 0.4966 - accuracy: 0.7456 - precision_7: 0.7385 - recall_7: 0.7103 - val_loss: 0.5014 - val_accuracy: 0.7436 - val_precision_7: 0.7409 - val_recall_7: 0.7135\n",
            "Epoch 5/5\n",
            "151978/151978 [==============================] - 218s 1ms/step - loss: 0.4858 - accuracy: 0.7529 - precision_7: 0.7432 - recall_7: 0.7165 - val_loss: 0.4996 - val_accuracy: 0.7460 - val_precision_7: 0.7454 - val_recall_7: 0.7182\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fbd86118588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pi5Y1shK3vy"
      },
      "source": [
        "Сильного улучшения не наблюдается, однако все же метрики на валидационной выборке немного повысились: accuracy на 1 процент, точность на и полнота на чуть меньше. Возможно, если подождать еще несколько эпох, то значения будут еще выше. "
      ]
    }
  ]
}